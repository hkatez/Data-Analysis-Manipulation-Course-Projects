---
output:
  html_document: default
  pdf_document: default
---
SI 618 Homework 9
=====================

## Homework Overview
The purpose of this homework is to give you more experience with data aggregation in R, visualization using ggplot() and time-series analysis on a real-world dataset. You will be analyzing Wikipedia page popularity over the past few years. The dataset is from (https://www.kaggle.com/c/web-traffic-time-series-forecasting/data), but we have provided the CSV (page_views.csv) for you. We have also provided an example document (si618_hw9_solution.html) of the intended solution.

## Question 0: Loading data and preprocessing (5 points)

Load the data (page_views.csv), convert to data.table and remove NA. (There are many ways to handle missing data, we choose to remove rows for the purpose of this assignment.) 
```{r, echo=FALSE, warning=FALSE}
library(data.table)
pv <- read.csv(file = "page_views.csv", header=TRUE, sep = ",")
pv <- data.table(pv)
pv <- na.omit(pv)
```


## Question 1: Average Popularity Analysis (20 points)

#### Question 1-a: Plot the distribution of the average popularity of pages. (10 points)

Hint: You can use stat_ecdf of ggplot to plot a CDF (cumulative distribution function) 

Hint2: You can use the scales library and use the comma format for the labels for the x-axis to be similar to what is produced here.

Your plot should look something like this. Note that there is one data point that will be removed while creating this plot. This will happen due to the use of logarithmic scale. You can handle that in different ways. Given that this only affects one row, in this case we chose to ignore it. You can plot this distribution in linear scale first to see why logairthmic scale is needed.
```{r, echo=FALSE, warning=FALSE}
library(ggplot2)
pv$avg_pop <- rowMeans(pv[, 2:551], na.rm = FALSE, dims = 1)
ggplot(pv, aes(avg_pop)) + stat_ecdf(geom = "step") + scale_x_log10() + xlab("Average Popularity Across Days") + ylab("Fraction of Articles (ECDF)")
```


#### Question 1-b: What are the top-10 and bottom-10 pages with regards to popularity? (10 points)
```{r, echo=FALSE, warning=FALSE}
#library(tidyverse)
library(dplyr)

page_top <- pv[, .(avg_pop), by = Page]
page_top <- page_top %>% arrange(desc(avg_pop))
head(page_top, 10)

page_bottom <- page_top %>% arrange(avg_pop)
head(page_bottom, 10)
```


## Question 2: Day-of-the-week Effect (15 points)

Is there a day-of-the-week effect on Wikipedia? On average, which day sees the most Wikipedia views (total across all articles)? What about the least views?
```{r, echo=FALSE, warning=FALSE}
library(lubridate)
col_name <- colnames(pv[, c(2:551)])
col_sum <- data.table(colSums(pv[, c(2:551)]))
dayviews <- cbind(date = col_name, views = col_sum)
dayviews[, views:=as.numeric(views.V1)]

dayviews$date <- gsub("X", "",dayviews$date)
dayofweek <- dayviews[, .(day=weekdays(ymd(date)), views)]
dayofweek[, .(views=mean(views)), by = day][order(-views), ]

```

## Question 3: Wikipedia views per day (30  points)

Plot Wikipedia views per day (total across all pages) over time. Then plot the de-seasonified data where the day-of-the-week effects are removed. Put those in the same plot using grid.arrange. (Note: You will need to change the figure width to see the plots properly. You dont have to match the exact same width we have here, just make sure it is readable.) 

```{r, fig.width=10, echo=FALSE, warning=FALSE}
library(gridExtra)
original <- ggplot(dayviews, aes(x = ymd(date), y = views)) + geom_line() + ggtitle("Original") + xlab("Date")+ ylab("Views")

de_seasonified <- function(var, day){resid(lm(var ~ factor(day), na.action = "na.exclude")) + mean(var, na.rm = TRUE)}
re <- dayviews[, .(day=weekdays(ymd(date)), views, date)][, "vn" := list(de_seasonified(views, day))]
dow_removed <- ggplot(re, aes(ymd(date), vn)) + geom_line() + ggtitle("Day-of-Week Effect Removed") + xlab("Date") + ylab("Views") 
grid.arrange(original, dow_removed, top="Wikipedia Views Per Day", nrow = 1)
```

## Question 4: Repeat Q3 for the top 50K articles (30 points)

Repeat Q3 for the top 50K articles. For this you will need to first find the top 50K pages and limit your dataset to those. We define these top pages as those with the largest number of overall page views. Next you need to find the day of the week effect for this subset and plot the two time series.

```{r, fig.width=10, echo=FALSE, warning=FALSE}
pv$sum_pop <- rowSums(pv[, 2:551], na.rm = FALSE, dims = 1)
top_50k <- pv %>% arrange(desc(sum_pop))
top_50k <- top_50k %>% top_n(n = 50000, wt = sum_pop)

col_name_50k <- colnames(top_50k[, c(2:551)])
col_sum_50k <- data.table(colSums(top_50k[, c(2:551)]))
dayviews_50k <- cbind(date = col_name_50k, views = col_sum_50k)
dayviews_50k[, views:=as.numeric(views.V1)]

dayviews_50k$date <- gsub("X", "",dayviews_50k$date)
dayofweek_50k <- dayviews_50k[, .(day=weekdays(ymd(date)), views)]
dayofweek_50k <- dayofweek_50k[, .(views=mean(views)), by = day][order(-views), ]

original_50k <- ggplot(dayviews_50k, aes(x = ymd(date), y = views)) + geom_line() + ggtitle("Original") + xlab("Date")+ ylab("Views")

#de_seasonified <- function(var, day){resid(lm(var ~ factor(day), na.action = "na.exclude")) + mean(var, na.rm = TRUE)}
re_50k <- dayviews_50k[, .(day=weekdays(ymd(date)), views, date)][, "vn" := list(de_seasonified(views, day))]
dow_removed_50k <- ggplot(re_50k, aes(ymd(date), vn)) + geom_line() + ggtitle("Day-of-Week Effect Removed") + xlab("Date") + ylab("Views") 

grid.arrange(original_50k, dow_removed_50k, top="Wikipedia Views Per Day - Top 50K Articles", nrow = 1)
```


## What to submit?
A zip file named 'si618_hw9_youruniquename.zip' containing:

* The R Markdown file named 'si618_hw9_report_youruniquename.Rmd'
* The HTML or PDF report generated by your R Markdown file that should similar to si618_hw9_solution.html





