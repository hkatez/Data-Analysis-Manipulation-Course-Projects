---
title: "SI 618 Homework 10"
output:
  html_document: default
  pdf_document: default
---

In this homework, you'll practice using hierarchical and k-means-family clustering on an automotive dataset.  The file **cars.tsv** is provided in the zip file.  You'll also need to load the 'gplots' and 'cluster' libraries. Note that many clustering-related functions in R either produce graphical output themselves, or produce objects that work directly with R's built-in plot() function. So unlike other assignments, you don't need to use ggplot() here to create these plots: just plot() will do.

## Part 1. Data preparation (20 points)

To prepare for clustering, you need to scale the data: Do this for the **cars** dataset by calling the appropriate R scaling function: use settings so that each variable (column) is centered by subtracting the variable (column) mean, and scaled by dividing by the variable's standard deviation. Use the car names for the data table row names.

(a) Show the first 5 rows of the scaled data frame, and 

```{r, echo=FALSE, fig.width=14, message=FALSE, warning=FALSE}
cardata <- read.table("cars.tsv", 
                   header = TRUE, fill = TRUE,sep="\t",quote = "", na.strings = "")
cars.data = cardata[c(3:8)]
data_scale = scale(cars.data)


head(cars.data,5)

```


(b) Compute a distance object based on the Manhattan distance between the rows of the scaled dataset. Convert the distance object to a matrix and show the 5x5 upper corner of the matrix (i.e. containing the first 5 rows and columns).

```{r, echo=FALSE, fig.width=14}
cars.dist = dist(cars.data, method = "manhattan")
as.matrix(cars.dist)[1:5,1:5]


```

## Part 2. Hierarchical clustering. (20 points) 
Using the distance object you computed from 1(b), compute and plot a hierarchical cluster analysis using average-linkage clustering. With this clustering, cut the tree into 3 clusters and plot the dendogram with red borders around the clusters (Hint: use rect.hclust() function).

```{r, echo=FALSE,fig.width=14}

cars.hclust <- hclust(cars.dist, method = "average")
fit <- kmeans(cars.data,3)
plot(cars.hclust, labels = cardata$Car, main = 'Hierarchical cluster analysis using average linkage')
rect.hclust(cars.hclust, k = 3, border = 2)


```

## Part 3. Using clustering results (10 points)

The output from the tree-cutting function in 2(b) above should produce a mapping of car type to cluster number (from 1 to 3), like this:
```{r, echo=TRUE}
groups.3 <- as.integer(cutree(cars.hclust, k=3))
groups.3
```

With this group mapping, produce three tables:

a) a 1-dimensional contingency table showing the number of cars in each cluster;

b) a 2-dimensional contingency table of the number of cars in each cluster from each country of manufacture; and

c) a table showing the median value per cluster of each variable.

The desired output is shown here:

```{r, echo=FALSE}
library(data.table)

table(groups.3)
table(groups.3,cardata$Country)
aggregate(cars.data,list(groups.3),median)


```

## Part 4. Heatmaps (10 points)

Use the heatmap.2 function to produce a heatmap of the cars dataset with these settings:

- average-link clustering

- column-based scaling

- row-based dendrogram

- no density info

You do not need to reproduce the exact width and height shown here, but for reference the example used these settings:

margins = c(5, 8), cexRow=0.7,cexCol=0.7.

```{r, echo=FALSE, fig.width=10}
library(gplots)
heatmap.2(as.matrix(data_scale), 
hclustfun = function(x) 
	hclust(x,method = "average"), 
scale = "column", 
dendrogram="row", 
trace="none", 
density.info="none", 
col=redblue(256), 
lhei=c(2,5.0), lwid=c(1.5,2.5), 
keysize = 0.25, 
margins = c(5, 8), 
cexRow=0.7,cexCol=0.7)


```

## Part 5. k-medoids clustering. (20 points)

Apply the `partitioned around medoids' R function to the distances you computed in 1(b) to find three clusters of cars.  

(a) Compare this to the 3 clusters you found with heirarchical clustering in Part 2, by showing the 2-dimensional contingency table for the hierarchical group variable (shown in Part 3) vs. the clustering variable that is output by the 'partitioned around medoids' function (Part 4).  How well do the two clusterings agree?  (**include your answers as output in your version of this report**)

```{r, echo=FALSE, fig.height = 10, fig.width=10}
library(cluster)
pam = pam(data_scale, k = 3 )
table(groups.3, pam$clustering)
print("The two clusterings gets the same number for each group")

```

(b) Give the medoid car found for each cluster. (**include your answers as output in your version of this report**)

```{r, echo=FALSE, fig.height = 10, fig.width=10}
cardata$Car[pam$id.med]
print("The medoid car found for each cluster is Dodge St Regis, Dodge Omni, Ford Mustang Ghia")

```

(c) Show the k-medoids clusters from 5(a) using the appropriate bivariate cluster plotting function, as shown.

```{r, echo=FALSE, fig.height = 10, fig.width=10}
library(factoextra)
clusplot(pam,main = "k-medoid clustering of cars into 3 groups",labels= 2)


```

## Part 6. Assessing cluster quality. (15 points)

Create a silhouette plot based on the k-medoid clusters found in Part 5 and distance matrix from Part 1. 
What can you conclude from the plot about the quality of these three clusters? (Include your answer as output in your version of this report.)
```{r}
plot(silhouette(pam(cars.dist,k=3)),main = "Silhouette plot of three car clusters")
print("The quality is good because the values are all positive and not close to zero")

```

## What to submit:
A zip file named 'si618_hw10_youruniquename.zip' containing:

* The R Markdown file you wrote, named 'si618_hw10_youruniquename.Rmd'
* The html report generated by your R Markdown file. 

